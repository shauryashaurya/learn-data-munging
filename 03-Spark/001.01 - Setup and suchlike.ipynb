{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "382d37a0-4ed1-4bed-95c6-ba27b449a843",
   "metadata": {},
   "source": [
    "# Setting up Spark, PySpark etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c98f438-3c7b-4a2f-a8d7-8a6ceab559fa",
   "metadata": {},
   "source": [
    "<font color='green'>__Support for Google Colab__  </font>\n",
    "\n",
    "<font color='green'>Look for the \"_Sidebar_: Google Colab\" section below to setup and run this Spark notebook on Google Colab.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea36fbb-47bf-4dc9-98bb-e40baa06fdcb",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Installing Spark locally (Windows)\n",
    "\n",
    "Getting ```Spark```, ```pySpark``` and other libraries to run is not that difficult, but there's a bit of tedium involved, also it changes from system to system slightly.  \n",
    "\n",
    "The following are a 'best-guess' set of instructions that largely seem to work on Windows 11.  \n",
    "\n",
    "Your mileage may vary.  \n",
    "\n",
    "Once you are through the installation and setup - it gets good. I promise. :)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68449ac-f67d-47ce-9e08-e0ec91aab242",
   "metadata": {},
   "source": [
    "## Steps involved setting up a local install of Spark\n",
    "\n",
    "1. download the key stuff - JDK, Spark\n",
    "2. get WinUtils (this is your Hadoop on Windows)\n",
    "3. setup system paths\n",
    "4. test\n",
    "\n",
    "### Pyspark\n",
    "1. Pyspark seems to come with it's own bundled spark distribution so you don't need anything else\n",
    "2. I'd still install Anaconda (or Miniconda) - just so I have the rest of the python modules needed (like Jupyter etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f0e372-ccdf-4e43-a0c6-80584372ed39",
   "metadata": {},
   "source": [
    "## Download (and install where applicable)\n",
    "* JDK (more open the better)\n",
    "* Hadoop (3.3.x, at this time) - _for windows, we just need Hadoop Winutils_\n",
    "* [Hadoop *winutils* (corresponding to the version of Hadoop)](https://github.com/cdarlint/winutils), [another repo](https://github.com/kontext-tech/winutils)\n",
    "* [Spark (3.x, at this time)](https://spark.apache.org/downloads.html)  \n",
    "* [Anaconda - Open Source/Individual Edition](https://www.anaconda.com/products/distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83b050b-3868-422d-b1ea-6d343406a9b6",
   "metadata": {},
   "source": [
    "## Setup environment variables \n",
    "\n",
    "We set these environment variables that help manage paths better.\n",
    "Example variable values would look like:\n",
    "Java:  \n",
    "* JAVA_HOME = ```C:\\[Java]```  \n",
    "    \n",
    "Hadoop:  \n",
    "* HADOOP_HOME = ```C:\\hadoop\\hadoop-3.4.0-win10-x64```  \n",
    "_this is just a sample_  \n",
    "_On Windows, we need HADOOP_HOME to be the folder where **winutils.exe** is located_  \n",
    "_For dev purposes: you can just download winutils and move on, just ensure you point to winutils.exe in this env variable_\n",
    "\n",
    "finally, Spark:   \n",
    "* SPARK_HOME = ```C:\\Spark\\spark-3.4.1-bin-hadoop3```  \n",
    "\n",
    "*notice there are no backslashes in the end. This is because slashes will be added in the next step when we setup path*      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9eb41d7-1ed7-4957-bb14-a366c3f456f0",
   "metadata": {},
   "source": [
    "## Update system **'PATH'**\n",
    "\n",
    "We use the variables defined above to set-up paths.  \n",
    "\n",
    "* Java: ```%JAVA_HOME%/bin```\n",
    "* Hadoop 01: ```%HADOOP_HOME%/bin``` \n",
    "* [*update: as of Hadoop 3.3.6, this does not seem to be necessarry*] Hadoop 02: ```%HADOOP_HOME%/sbin``` (*sbin needed in addition to bin*)\n",
    "* Spark: ```%SPARK_HOME%/bin```  \n",
    "    \n",
    "(*here we add backslashes before bin*)\n",
    "\n",
    "N.B.: If you are only doing the dev setup (just winutils and nothing else) thing, you may not need the Hadoop 01/02 above, I added those to my system anyway."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6f1786-6a46-430e-9c18-c116a7bfde7c",
   "metadata": {},
   "source": [
    "## Patch Hadoop  \n",
    "\n",
    "This is *needed* when Hadoop is run on Windows.\n",
    "\n",
    "* copy the ```bin``` folder from the right version of winutils to replace ```%HADOOP_HOME%/bin```  \n",
    "\n",
    "* copy ```hadoop-yarn-server-timelineservice-3.0.3``` from ```%HADOOP_HOME%\\share\\hadoop\\yarn\\timelineservice``` to ```%HADOOP_HOME%\\share\\hadoop\\yarn``` (the parent directory).  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fd6f2c-9f49-4933-a42a-9db1b9717922",
   "metadata": {},
   "source": [
    "## Install the Python libraries\n",
    "\n",
    "Prefer installing [Anaconda](https://www.anaconda.com/products/distribution). \n",
    "It resolves other dependencies like Pandas, Numpy, Jupyter etc. too.  \n",
    "Once there, use either pip or conda - they are both cool but incompatible.  \n",
    "The conda-forge channel is a few days behind the pip one.  \n",
    "We're only running on the local machine here, no complicated infrastructure to care about.  \n",
    "So, you do you.  \n",
    "\n",
    "Use one of the following commands (from the command line obvs) to install each:  \n",
    "* pyspark:\n",
    "    * ```pip install pyspark``` or\n",
    "    * ```conda install -c conda-forge pyspark```\n",
    "* findspark:\n",
    "    * ```pip install findspark``` or\n",
    "    * ```conda install -c conda-forge findspark```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc84fa3e-3562-4390-b11a-a909372cca3c",
   "metadata": {},
   "source": [
    "## (_Optionally_) Configure Hadoop\n",
    "\n",
    "*only needed if you want to use hadoop as your file storage system*  \n",
    "\n",
    "* create a folder for ```namenode```\n",
    "* create a folder for ```datanode```\n",
    "* four files: ```core-site.xml```, ```mapred-site.xml```, ```hdfs-site.xml```, ```yarn-site.xml``` - see code for each in the [reference repo](https://github.com/MuhammadBilalYar/Hadoop-On-Window) above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86452ab-a083-4c37-90b9-f7bec7f49f90",
   "metadata": {},
   "source": [
    "## Caveats\n",
    "\n",
    "The scope of these notebooks is *usage* - not setup or troubleshooting, am pretty sure these installation instructions will be outdated soon and be replaced by pre-built docker images or shell scripts or automated installs for windows or such-like. Google, DDG - your best friends my friend :)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfef3199-6aa7-48e1-a19a-c1cf603929da",
   "metadata": {},
   "source": [
    "## References  \n",
    "\n",
    "* [PySpark installation - Spark Documentation](https://spark.apache.org/docs/latest/api/python/getting_started/install.html)\n",
    "* [How to install Hadoop on Win 10](https://muhammadbilalyar.github.io/blogs/How-to-install-Hadoop-on-Window-10/)\n",
    "* [Hadoop on Windows](https://github.com/MuhammadBilalYar/Hadoop-On-Window)\n",
    "* [Hadoop and Spark on Windows](https://dev.to/awwsmm/installing-and-running-hadoop-and-spark-on-windows-33kc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840529a7-7cb0-41ad-9812-3287bf36d7ce",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "This boiler plate helps, esp. in Jupyter Lab / Notebook situations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d1f448-ff2f-4020-b336-08adb398414f",
   "metadata": {},
   "source": [
    "## _Sidebar_: Google Colab\n",
    "\n",
    "You don't need to run this on your local machine.\n",
    "The notebook is setup to run on Google Colab as well.\n",
    "\n",
    "For a detailed description of how this is setup, see the [How to run Apache Spark based notebooks in Google Colab](https://github.com/shauryashaurya/learn-data-munging/blob/main/03-Spark/001.02%20(optional)%20Google_Colab_setup_Spark_download_data.ipynb) notebook\n",
    "\n",
    "Open the notebook in Google Colab using the following button, then uncomment the setup marked # SETUP FOR COLAB  \n",
    "  \n",
    "  \n",
    "<a href=\"https://colab.research.google.com/github/shauryashaurya/learn-data-munging/blob/main/03-Spark/001.02%20(optional)%20Google_Colab_setup_Spark_download_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>    \n",
    "\n",
    "_NOTE: keep the # SETUP FOR COLAB step below commented (disabled) when you are running this notebook locally_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14b76335-8557-418b-be9e-4aa4acd77623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # SETUP FOR COLAB: select all the lines below and uncomment (CTRL+/ on windows)\n",
    "\n",
    "# # grab spark\n",
    "# # as of 2025-01-18, the latest version is 3.5.4, get the link from Apache Spark's website\n",
    "# ! wget -q https://www.apache.org/dyn/closer.lua/spark/spark-3.5.4/spark-3.5.4-bin-hadoop3.tgz\n",
    "# # unzip spark\n",
    "# !tar xf spark-3.5.4-bin-hadoop3.tgz\n",
    "# !pip install pyspark\n",
    "# !pip install plotly\n",
    "# !pip install -q findspark\n",
    "# # install findspark package\n",
    "# !pip install -q findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e64cb5c-3a1f-482f-b284-d8741188f059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "import os, sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c872aa69-95d4-4726-b81a-fa1a11fce443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # got to provide JAVA_HOME and SPARK_HOME vairables\n",
    "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
    "# os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.4-bin-hadoop3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7af26d63-2fdb-4761-9323-99754af9ab79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the necessary environment variables\n",
    "\n",
    "os.environ[\"PYSPARK_PYTHON\"] = sys.executable\n",
    "# os.environ[\"\n",
    "# os.environ[\"PYSPARK_PYTHON\"] = os.getenv('SPARK_HOME') + \"\\\\python\\\\lib\"\n",
    "os.environ[\"spark_python\"] = os.getenv('SPARK_HOME') + \"\\\\python\"\n",
    "os.environ[\"py4j\"] = os.getenv('SPARK_HOME') + \"\\\\python\\\\lib\\\\py4j-0.10.9.7-src.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cc06a5b5-e7d7-4135-bb78-59e89d25ce44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: initialize findspark\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "99472ee8-1a32-4ff8-b24b-2e14ea688711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: import pyspark\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0b3c408d-6a9c-4360-80af-d13d9bfb3742",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.5.4'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyspark.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bfd85fe8-626a-4d3b-aafe-db9b4287955c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Create a spark session\n",
    "\n",
    "# 'local[1]' indicates spark on 1 core on the local machine, specify the number of cores needed\n",
    "# use .config(\"spark.some.config.option\", \"some-value\") for additional configuration\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master('local[1]') \\\n",
    "    .appName(\"10+ minutes to pyspark\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fea1e3f1-970b-45ef-a6ad-3dec4122881e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://The-Wonder-Of-U:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[1]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>10+ minutes to pyspark</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1857ed223f0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28c5cd6-4e7e-4ffc-8004-529d8d834fe2",
   "metadata": {},
   "source": [
    "Back in the day you'd need various 'contexts' as entry points into spark functionality.  \n",
    "All of this is now wrapped into a SparkSession, easy to manage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f3a626e1-b87d-40f6-8bf6-27002fa2bdf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hack when you get Py4J security exceptions on Windows 11\n",
    "# spark.conf.set(\"spark.driver.extraJavaOptions\", \"-Dio.netty.tryReflectionSetAccessible=true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5813036d-5040-4b89-9096-c64a2c6b63c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://The-Wonder-Of-U:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[1]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>10+ minutes to pyspark</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[1] appName=10+ minutes to pyspark>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The SparkSession carries the sparkContext\n",
    "spark.sparkContext\n",
    "# sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2261514-fc8e-4212-94fc-12a9094f770f",
   "metadata": {},
   "source": [
    "Check out the spark UI link when you uncomment the lines in the two cells above.  \n",
    "Your local UI should launch at a link like: http://localhost:4040/jobs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e5fc530a-f525-4859-94aa-e5b151e54462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# before we close the notebook, stop spark, otherwise Jupyter closes, but scala-spark keep going on...\n",
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100f1cc4-d808-4462-9b29-b0e11b524c19",
   "metadata": {},
   "source": [
    "# Next"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b2e342-1c3f-4460-88e3-b7ce58e96adc",
   "metadata": {},
   "source": [
    "* There's an optional notebook that walks you through the Google Colab setup in more detail\n",
    "* A quick ```10+ minutes to PySpark``` notebook for a refresher on the syntax and idioms\n",
    "* We start data analysis on MovieLens in the earnest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6941ce3e-5115-4594-882c-58b7ae4f484c",
   "metadata": {},
   "source": [
    "# Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f5760d-4e43-4257-b527-0d98cac7ac78",
   "metadata": {},
   "source": [
    "## DataFrames: Create and View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8112d050-d2cc-4cf6-8f3b-13b86a533fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, date\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyspark.sql\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4adb4a71-555c-45cb-9be6-2c954e008ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "r1 = Row(a=1,b=2.,c='span a',d=date(2022,7,1),e=datetime(2022,7,1,12,0))\n",
    "r2 = Row(a=1,b=3.,c='can a ',d=date(2022,7,2),e=datetime(2022,7,2,12,0,1))\n",
    "r3 = Row(a=1,b=4.,c='banana',d=date(2022,7,3),e=datetime(2022,7,3,12,0,2))\n",
    "r4 = Row(a=1,b=5.,c='manana',d=date(2022,7,4),e=datetime(2022,7,4,12,0,3))\n",
    "rows = [r1,r2,r3,r4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5c4cf2d5-cc89-42be-b54c-deffab550ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a list of pyspark.sql.Row\n",
    "df1 = spark.createDataFrame([r1,r2,r3,r4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7b66b631-103a-4118-8601-9765bc75acb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[a: bigint, b: double, c: string, d: date, e: timestamp]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b2547bac-b919-47cd-83ae-03d4c0f1656a",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o83.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1) (The-Wonder-Of-U executor driver): java.io.IOException: Cannot run program \"C:\\1\\spark\\spark-3.5.4-bin-hadoop3\\python\\lib\": CreateProcess error=5, Access is denied\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1143)\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1073)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:181)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\nCaused by: java.io.IOException: CreateProcess error=5, Access is denied\r\n\tat java.base/java.lang.ProcessImpl.create(Native Method)\r\n\tat java.base/java.lang.ProcessImpl.<init>(ProcessImpl.java:494)\r\n\tat java.base/java.lang.ProcessImpl.start(ProcessImpl.java:159)\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1110)\r\n\t... 34 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3316)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3316)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3539)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\nCaused by: java.io.IOException: Cannot run program \"C:\\1\\spark\\spark-3.5.4-bin-hadoop3\\python\\lib\": CreateProcess error=5, Access is denied\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1143)\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1073)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:181)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\t... 1 more\r\nCaused by: java.io.IOException: CreateProcess error=5, Access is denied\r\n\tat java.base/java.lang.ProcessImpl.create(Native Method)\r\n\tat java.base/java.lang.ProcessImpl.<init>(ProcessImpl.java:494)\r\n\tat java.base/java.lang.ProcessImpl.start(ProcessImpl.java:159)\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1110)\r\n\t... 34 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# df1's not been evaluated yet. It's lazy evaluation\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# to eval it, we go\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m df1\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32mC:\\1\\spark\\spark-3.5.4-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\sql\\dataframe.py:947\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    887\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    888\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Prints the first ``n`` rows to the console.\u001b[39;00m\n\u001b[0;32m    889\u001b[0m \n\u001b[0;32m    890\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    945\u001b[0m \u001b[38;5;124;03m    name | Bob\u001b[39;00m\n\u001b[0;32m    946\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_show_string(n, truncate, vertical))\n",
      "File \u001b[1;32mC:\\1\\spark\\spark-3.5.4-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\sql\\dataframe.py:965\u001b[0m, in \u001b[0;36mDataFrame._show_string\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    959\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[0;32m    960\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    961\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[0;32m    962\u001b[0m     )\n\u001b[0;32m    964\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[1;32m--> 965\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mshowString(n, \u001b[38;5;241m20\u001b[39m, vertical)\n\u001b[0;32m    966\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    967\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\1\\a\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mC:\\1\\spark\\spark-3.5.4-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mC:\\1\\a\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o83.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1) (The-Wonder-Of-U executor driver): java.io.IOException: Cannot run program \"C:\\1\\spark\\spark-3.5.4-bin-hadoop3\\python\\lib\": CreateProcess error=5, Access is denied\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1143)\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1073)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:181)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\nCaused by: java.io.IOException: CreateProcess error=5, Access is denied\r\n\tat java.base/java.lang.ProcessImpl.create(Native Method)\r\n\tat java.base/java.lang.ProcessImpl.<init>(ProcessImpl.java:494)\r\n\tat java.base/java.lang.ProcessImpl.start(ProcessImpl.java:159)\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1110)\r\n\t... 34 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3316)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3316)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3539)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\nCaused by: java.io.IOException: Cannot run program \"C:\\1\\spark\\spark-3.5.4-bin-hadoop3\\python\\lib\": CreateProcess error=5, Access is denied\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1143)\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1073)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:181)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\t... 1 more\r\nCaused by: java.io.IOException: CreateProcess error=5, Access is denied\r\n\tat java.base/java.lang.ProcessImpl.create(Native Method)\r\n\tat java.base/java.lang.ProcessImpl.<init>(ProcessImpl.java:494)\r\n\tat java.base/java.lang.ProcessImpl.start(ProcessImpl.java:159)\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1110)\r\n\t... 34 more\r\n"
     ]
    }
   ],
   "source": [
    "# df1's not been evaluated yet. It's lazy evaluation\n",
    "# to eval it, we go\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd0fa9b-1d38-4ad4-adbd-ff0099c9722e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a list of tuples with explicit schema\n",
    "df2 = spark.createDataFrame(\n",
    "    [\n",
    "        (2,5.,'man a',date(2022,7,1),datetime(2022,7,1,12,0)),\n",
    "        (2,6.,'can a',date(2022,8,1),datetime(2022,7,2,12,0)),\n",
    "        (2,7.,'manna',date(2022,9,1),datetime(2022,7,3,12,0))\n",
    "    ],\n",
    "    schema = 'a bigint, b double, c string, d date, e timestamp',\n",
    "    verifySchema = True\n",
    ")\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a24a3ef-168d-485f-b4dc-eaa7017c3e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use spark RDDs to create a dataframe\n",
    "# go to the sparksession's sparkContext to access parallelize method\n",
    "rdd3 = spark.sparkContext.parallelize(\n",
    "    [\n",
    "        (3,5., 'main', date(2022,7,1), datetime(2022,7,1,12,0,1)),\n",
    "        (3,5., 'brain', date(2022,7,1), datetime(2022,7,1,12,0,1)),\n",
    "        (3,5., 'pain', date(2022,7,1), datetime(2022,7,1,12,0,1))\n",
    "    ]\n",
    ")\n",
    "\n",
    "df3 = spark.createDataFrame(rdd3, schema=['a','b','c','d','e'], verifySchema = True)\n",
    "\n",
    "# df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197b141d-c161-47dc-98d3-f72df087a1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can also use a pandas dataframe to create a spark dataframe\n",
    "df4_pd = pd.DataFrame(\n",
    "    {\n",
    "        'a': np.random.randint(0,10, size = 3),\n",
    "        'b': np.random.randn(3),\n",
    "        'c': [\"gandalf's manager\", \"said\", 'no'],\n",
    "        'd': [date(2022,7,1),date(2022,7,2),date(2022,7,3)],\n",
    "        'e': [datetime(2022,7,1,12,0,1),datetime(2022,7,2,12,0,2),datetime(2022,7,3,12,0,3)]\n",
    "    }\n",
    ")\n",
    "\n",
    "df4 = spark.createDataFrame(df4_pd)\n",
    "\n",
    "df4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb5d002-f635-444b-bf9f-f99fef8d8245",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da175024-a73a-48db-9fd0-781426884f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use printSchema() to..., you know, it says what it does\n",
    "# also don't you hate that pySpark is following the Java camelCase instead of the Python snake_case?\n",
    "# yeah, what's that all about?\n",
    "\n",
    "df1.printSchema()\n",
    "df2.printSchema()\n",
    "df3.printSchema()\n",
    "df4.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8da638-93f0-40f4-9c04-1135a150ff10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show only x rows\n",
    "df1.show(1)\n",
    "# vertical - if the row is too long for horizontal display\n",
    "df4.show(2, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c13f4ec-236e-4c07-9663-7a6ea8c0fd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect() - collects the entire df from across all nodes to the driver\n",
    "# if you don't have enough memory, here's how you crash spark\n",
    "# careful is the word\n",
    "df3.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5f0199-9653-4ac9-9aea-d93d90ab810b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas used to have a take() method, deprecated now\n",
    "# take() in spark extracts the first n rows of a dataframe\n",
    "df2.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e549afe1-56a2-4d6e-a0d4-8027ad8c3adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# same as take() but returns the last n rows of a dataframe\n",
    "df2.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481a880f-b99a-4f5e-bde0-00e9fce26439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hey you want another way of crashing the spark driver?\n",
    "# convert the spark dataframe to a pandas dataframe\n",
    "# it'll collect all data from all workers into the driver\n",
    "\n",
    "# this naive approach will fail because we have date-time values in df3 . \n",
    "# df3.toPandas()\n",
    "# possible error: TypeError: Casting to unit-less dtype 'datetime64' is not supported. Pass e.g. 'datetime64[ns]' instead.\n",
    "\n",
    "# explicitly provide date-time data type to Pandas\n",
    "from pyspark.sql.functions import date_format\n",
    "df3.withColumn(\"d\", date_format(\"d\", \"yyyy-MM-dd HH:mm:ss\")) \\\n",
    "    .withColumn(\"e\", date_format(\"e\", \"yyyy-MM-dd HH:mm:ss\")) \\\n",
    "    .toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd07387-7ed4-4ad9-aecc-671838c56d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how do I make it so my dataframes are evaluated eagerly?\n",
    "# instead of the regular lazy eval?\n",
    "# y'know when am in notebooks and stuff?\n",
    "# set the config\n",
    "spark.conf.set('spark.sql.repl.eagerEval.enabled', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8481b77f-6326-4903-8ac9-fc9da71c2e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# there you go, eager evaulatio'\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e71e5fc-ec27-4ad1-94a0-22bcbdb6b0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set it back to false if you like\n",
    "spark.conf.set('spark.sql.repl.eagerEval.enabled', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22802f5-0748-441a-bbd1-aa7b342823e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No more eagerly evaluated dataframes\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d269bcec-f5af-404e-a18e-d375d13fa9e6",
   "metadata": {},
   "source": [
    "# Selecting and Accessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb46f3aa-8069-4210-ae1a-32d9c04cc843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# access a column\n",
    "df1.a, df2.b, df3.c, df4.d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73dc9937-6bb5-499d-847a-15f54e9166e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Column\n",
    "from pyspark.sql.functions import upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321e8580-f33d-4e22-82b7-4ceb343e179c",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df1.c) == type(upper(df1.c)) == type(df1.c.isNull())\n",
    "# TODO: what's going on with type(df1.c.isNull()) above???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ef492d-8bf5-4b0c-b1bd-b861f5ad2ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.c.isNull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e413ce30-1ee8-4f65-8d69-a6727002f9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use dataframe's select method to identify a column and show() it\n",
    "df1.select(df1.c).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d6dd44-09cd-489d-a411-ccec9700650e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# also there's dataframe.filter()\n",
    "df4.filter(df4.a>0).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e2f640-c765-4563-b374-59b067e55d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e223c677-cd46-450a-8b5a-c840d829abd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign a new column instance to the dataframe\n",
    "df4_withNewCol = df4.withColumn('upper_c', upper(df4.c))\n",
    "df4_withNewCol.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7be7e54-6816-41bc-8a16-28c495c9c72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.show()\n",
    "df4_withNewCol.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81243886-7261-41a0-890f-d81682015ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter\n",
    "df4.filter(df4.a == 9).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21a461e-ebc7-4bee-969f-8d010dc2c759",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.filter(df4.b > 0).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200e330c-cdd1-4c03-a39b-d688c67a8917",
   "metadata": {},
   "source": [
    "# UDFs: Applying a Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3990e598-5c94-4281-bfd0-5c0309c86c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "\n",
    "@pandas_udf('long')\n",
    "def pandas_plus_one(series: pd.Series) -> pd.Series:\n",
    "    #     plus one using pandas series\n",
    "    return series+1\n",
    "\n",
    "df4.select(df4.a, pandas_plus_one(df4.a)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b068a69-ee78-4af5-baa5-f5928feffd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pandas_filter(iterator):\n",
    "    for pandas_df in iterator:\n",
    "        yield pandas_df[pandas_df.b>0]\n",
    "        \n",
    "df4.mapInPandas(pandas_filter, schema = df4.schema).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4733bfe0-16ed-423a-be48-d2e32a6c40b3",
   "metadata": {},
   "source": [
    "# Grouping Data\n",
    "\n",
    "Split-Apply-Combine, just like pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cadc92f-43e3-4e41-9014-45a253592a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by fruit, color etc.\n",
    "df5 = spark.createDataFrame([\n",
    "    ['red', 'banana', 1, 10], ['blue', 'banana', 2, 20], ['red', 'carrot', 3, 30],\n",
    "    ['blue', 'grape', 4, 40], ['red', 'carrot', 5, 50], ['black', 'carrot', 6, 60],\n",
    "    ['red', 'banana', 7, 70], ['red', 'grape', 8, 80]], schema=['color', 'fruit', 'v1', 'v2'])\n",
    "\n",
    "df5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa0b7a1-f2b9-4e50-aa98-2ad50e9933c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df5.groupby('color').avg().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ffb3db-ef9c-4064-8a14-12e961cfb40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: How to get the deviations in a new column?\n",
    "def plus_mean(pandas_df):\n",
    "    return pandas_df.assign(v1=pandas_df.v1 - pandas_df.v1.mean())\n",
    "\n",
    "df5.groupby('color').applyInPandas(plus_mean, schema = df5.schema).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cfa78f-0c13-48fb-80fe-069561e2dbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Co-grouping and applying a function.\n",
    "\n",
    "df6 = spark.createDataFrame(\n",
    "    [\n",
    "        (20000101, 1, 1.0), \n",
    "        (20000101, 2, 2.0), \n",
    "        (20000102, 1, 3.0), \n",
    "        (20000102, 2, 4.0)\n",
    "    ],\n",
    "    ('time', 'id', 'v1')\n",
    ")\n",
    "\n",
    "df7 = spark.createDataFrame(\n",
    "    [\n",
    "        (20000101, 1, 'x'), \n",
    "        (20000101, 2, 'y')\n",
    "    ],\n",
    "    ('time', 'id', 'v2')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9dcadc-3d04-4fb8-b0fa-508c7895da49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def asof_join(l, r):\n",
    "    return pd.merge_asof(l,r,on='time', by='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3444cbc-266b-424d-b7d6-5869596842b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df6_gb = df6.groupby('id')\n",
    "df7_gb = df7.groupby('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c0210f-0ade-4ede-9a75-2ef3aba467a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "co_grp = df6_gb.cogroup(df7_gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce09cd45-d9a7-4de8-95b2-e74e4afdd418",
   "metadata": {},
   "outputs": [],
   "source": [
    "rslt = co_grp.applyInPandas(asof_join, schema='time int, id int, v1 double, v2 string')\n",
    "rslt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fea2b34-ec3a-4356-8e8b-431a158b9340",
   "metadata": {},
   "source": [
    "# Getting data in and out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bfdb35-7ce8-42ce-a2c3-8feb29714099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV\n",
    "# .mode('overwrite') can be taken out when needed.\n",
    "df5.write.mode('overwrite').csv('fruits.csv', header=True)\n",
    "spark.read.csv('fruits.csv', header=True).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc6dcf6-a9fc-424c-b0b7-d989e366d717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parquet\n",
    "# .mode('overwrite') can be taken out when needed.\n",
    "df5.write.mode('overwrite').parquet('fruits.parquet')\n",
    "spark.read.parquet('fruits.parquet').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebec259-c99c-4886-9e48-8aa19cc35f90",
   "metadata": {},
   "source": [
    "# Working with SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f3781d-5d36-4cdb-b71c-c8fa4255357a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df5.createOrReplaceTempView('tableA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6c4545-e507-49ee-a225-76c1fa73e165",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('select count(*) from tableA').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf191353-ca5a-4ed9-bf0d-32eeca3af4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UDFs in SQL\n",
    "# register and invoke\n",
    "@pandas_udf('integer')\n",
    "def add_one(s: pd.Series) -> pd.Series:\n",
    "    return s+1\n",
    "\n",
    "spark.udf.register('add_one', add_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3073781-ada6-45bc-bea5-80763b4fbd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('SELECT v1, add_one(v1) from tableA').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb12790c-af9e-44d9-a348-c56c1c573606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can mix/match sql expressions \n",
    "# for e.g. take the expressions from above\n",
    "\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "df5.selectExpr('add_one(v1)').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bff81d2-0917-41ce-b8b1-f2dfc6e680bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df5.select(expr('count(*)')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d954a2-ac00-422c-b99c-d0bf954a2d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "df5.select(expr('count(*)')>0).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b34aa22-5b63-449c-9392-37ad4a1066bb",
   "metadata": {},
   "source": [
    "# Pandas API on Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3777d5-9a81-4b30-8d5d-1c56a2c86a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyspark.pandas as ps\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b23963-1003-4d36-adf4-8bb6724b445a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a pandas on spark series\n",
    "\n",
    "s1 = ps.Series([1,2,3,np.nan,5,np.nan,7,8,9])\n",
    "s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de809155-a4f8-4df1-8a70-6bcbc98f560b",
   "metadata": {},
   "outputs": [],
   "source": [
    "psdf1 = ps.DataFrame(\n",
    "    {'a': [1, 2, 3, 4, 5, 6],\n",
    "     'b': [100, 200, 300, 400, 500, 600],\n",
    "     'c': [\"one\", \"two\", \"three\", \"four\", \"five\", \"six\"]\n",
    "    },\n",
    "    index=[10, 20, 30, 40, 50, 60]\n",
    ")\n",
    "\n",
    "psdf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5039ab8a-7bc2-43f1-a38d-eea941335e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a pandas dataframe and convert to pandas-spark\n",
    "dates1 = pd.date_range('20220101', periods=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05f60ef-90b9-4700-8c00-85aa37690f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf1 = pd.DataFrame(np.random.randn(6,4), index=dates1, columns = list('ABCD'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7ea286-08ff-4c02-80b5-b8b4b0497b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b24e73-f5ff-4646-9bca-66e2ecc04212",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf1.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66815ce-729b-47dd-ad2d-c6b2a3ad6ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create spark dataframe from the pandas dataframe\n",
    "\n",
    "# naive method breaks with pandas 2.x due to date-time column\n",
    "# psdf1 = ps.from_pandas(pdf1)\n",
    "\n",
    "# we follow a 2-step approach here:\n",
    "# first convert datetime to string\n",
    "pdf1.index = pdf1.index.astype(\"string\")\n",
    "psdf1 = ps.from_pandas(pdf1)\n",
    "\n",
    "# now cast index back to datetime64 \n",
    "psdf1.index = psdf1.index.astype(\"datetime64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7290c4d5-746a-4a56-9d98-6a9c90198f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a pandas-on-spark dataframe now\n",
    "type(psdf1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15c4935-b074-4001-ac26-f75448263806",
   "metadata": {},
   "outputs": [],
   "source": [
    "psdf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9484a40e-ae9f-4718-8cbf-0fc841cb0545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a SPARK df from a PANDAS df (we've done this before)\n",
    "# create a PANDAS-ON-SPARK df from a SPARK df (we've done this too...)\n",
    "\n",
    "sdf2 = spark.createDataFrame(pdf1)\n",
    "sdf2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7dda40c-0930-4f0f-b0e6-41a200d97794",
   "metadata": {},
   "outputs": [],
   "source": [
    "psdf2 = sdf2.pandas_api()\n",
    "# this works like a pandas df now...\n",
    "psdf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769d4530-5e9e-4bc2-8fbf-71ba5ccff595",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(psdf2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f8ae02-8fe6-429b-8c64-b5ffe0a268f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "psdf2.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf90a972-0559-4ef9-8db0-5373126c973b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas head()\n",
    "psdf2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6a52ab-703f-4cb2-b326-cb2cc71846ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "psdf2.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d51febb-c421-414d-93b8-21cafe479a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "psdf2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9877570-c97b-4ff8-a796-75a34dc638c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "psdf2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505236fb-830c-4641-a983-119212bace76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# warning - this breaks the driver due to memory - just like collect()\n",
    "# be careful\n",
    "psdf2.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7fd9e4-beda-400f-9b80-453142bcdf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transpose\n",
    "psdf2.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119d902f-d894-4445-a6ca-7b103ad06c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort index\n",
    "psdf2.sort_index(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a368c8-6221-47a6-8184-1c99dd8ef426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort values\n",
    "psdf2.sort_values(by='B', ascending = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720fb28c-4631-4693-b515-fd4ddfd8f7a1",
   "metadata": {},
   "source": [
    "# Missing Data - Pandas on Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76efe94d-cc86-4f1b-be53-28c32acd9f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you'll notice we are repeating a lot of the tasks \n",
    "# from 10 minutes to pandas \n",
    "pdf2 = pdf1.reindex(index=dates1[0:4], columns=list(pdf1.columns) + ['E'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e410224-4d52-4364-902a-6b0e48183683",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf2.loc[dates1[0]:dates1[1], 'E'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c088799e-4a8f-4c82-b9a7-b1df67d228cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# updated for pandas 2.x+\n",
    "# explicitly cast dates to string\n",
    "pdf2.index = pdf2.index.astype(\"string\")\n",
    "psdf3 = ps.from_pandas(pdf2)\n",
    "# convert index back to datetime64\n",
    "psdf3.index = psdf3.index.astype(\"datetime64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2d382a-ed9e-40b7-95d4-85801bf0949c",
   "metadata": {},
   "outputs": [],
   "source": [
    "psdf3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bd40d9-0a7b-41d8-9e02-1264096e536e",
   "metadata": {},
   "outputs": [],
   "source": [
    "psdf3.dropna(how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e9fdb2-e711-473c-b893-e0bbc8f8fe37",
   "metadata": {},
   "outputs": [],
   "source": [
    "psdf3.fillna(value=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82930b3-4356-445a-af2d-d946c2988dcb",
   "metadata": {},
   "source": [
    "# Operations - Pandas on Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53b7f77-e6f9-4eb5-bdcc-e57be98e5afc",
   "metadata": {},
   "source": [
    "## Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba81474b-6cce-4eaa-b584-134bbc94c1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "psdf1.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c027d1-e4cf-4892-8e99-99700b62c882",
   "metadata": {},
   "source": [
    "## Spark Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48241248-8ea4-461d-a34a-f4f3e29d7d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the current config value\n",
    "previous = spark.conf.get('spark.sql.execution.arrow.pyspark.enabled')\n",
    "previous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76eaf89f-200b-477c-9ac4-dc654439f334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use default index prevent overhead.\n",
    "ps.set_option('compute.default_index_type', 'distributed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77bf139-927e-4fdf-8923-8788503f0d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore warnings coming from Arrow optimizations\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42a9183-20f7-4fe4-b3b5-d82f6cabcde6",
   "metadata": {},
   "source": [
    "Let's toggle ```spark.sql.execution.arrow.pyspark.enabled``` to see the difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2be4a3-a124-4b5c-8f88-88b1202dda52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is supposed to be faster by an order of magnitude\n",
    "spark.conf.set('spark.sql.execution.arrow.pyspark.enabled', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8645e135-b2c9-48aa-95c6-bd233a585f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit ps.range(3000000).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8618f5f-d31e-4300-a9b3-15f017510ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# is supposed to be slower\n",
    "spark.conf.set('spark.sql.execution.arrow.pyspark.enabled', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1c5845-5454-4770-8836-c7cef584547a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit ps.range(3000000).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12833aee-b9de-401f-8c99-6fc321860e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put everything back as it was\n",
    "ps.reset_option('compute.default_index_type')\n",
    "spark.conf.set('spark.sql.execution.arrow.pyspark.enabled', previous)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6a13c7-1bb9-4626-a5c9-e6235a12fac3",
   "metadata": {},
   "source": [
    "# Grouping  \n",
    "  \n",
    "  \n",
    "It's the same split-apply-combine deal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00bb178-44a4-4d7e-bbe5-dbe046eed8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "psdf4 = ps.DataFrame({'A': ['foo', 'bar', 'foo', 'bar',\n",
    "                          'foo', 'bar', 'foo', 'foo'],\n",
    "                    'B': ['one', 'one', 'two', 'three',\n",
    "                          'two', 'two', 'one', 'three'],\n",
    "                    'C': np.random.randn(8),\n",
    "                    'D': np.random.randn(8)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc848e4-1250-4583-a204-ee7c8feb4b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "psdf4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451ecd42-e9c7-43ac-bfde-15c26a44a397",
   "metadata": {},
   "outputs": [],
   "source": [
    "psdf4.groupby('A').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e84a471-6b41-4f00-83b6-de0cc7baf77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "psdf4.groupby(['A', 'B']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af92b814-b13d-4843-bc6d-cb9240dae244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# same datetime nonsense with pandas v2.x is going to happen here...\n",
    "pser = pd.Series(np.random.randn(1000),\n",
    "                 index=pd.date_range('1/1/2000', periods=1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee8ad71-bf8d-4991-b1fb-0ab8802ccb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...so we cast the index as \"string\"\n",
    "pser.index = pser.index.astype(\"string\")\n",
    "psser = ps.Series(pser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c29d6f-3478-40d0-bf54-13bf518a86ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "psser = psser.cummax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e0c2f9-8711-4e47-a3c0-72a82524132e",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(psser.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd89b2f-651d-4df8-92a2-3f65887f1ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "psser.reindex(psser.index.astype(\"string\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae22bb5-01a5-46cb-b954-b1fb76616dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "psser.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41f9c90-609c-4a57-9b2d-dd2acc44a2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf4 = pd.DataFrame(np.random.randn(1000, 4), index=pser.index,\n",
    "                   columns=['A', 'B', 'C', 'D'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e477ea70-3b1b-4bed-9a9a-6f8a8b873832",
   "metadata": {},
   "outputs": [],
   "source": [
    "psdf5 = ps.from_pandas(pdf4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace235cf-790d-4643-9631-96d2b7b45dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "psdf5 = psdf5.cummax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f6ea21-286c-454d-a2f1-024de7e8a8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "psdf5.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c302fe52-5685-4cd0-94c4-a9d3a3593616",
   "metadata": {},
   "source": [
    "# Getting data in/out - Pandas on Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d406b2b8-c025-4575-8c50-204d34f30111",
   "metadata": {},
   "outputs": [],
   "source": [
    "psdf4.to_csv('./data/foo.csv')\n",
    "ps.read_csv('./data/foo.csv').head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ef7e99-b9c6-49bc-a720-4020d591c919",
   "metadata": {},
   "outputs": [],
   "source": [
    "psdf4.to_parquet('./data/bar.parquet')\n",
    "ps.read_parquet('./data/bar.parquet').head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca29d52-9b7e-481a-8d4c-2246c44023f5",
   "metadata": {},
   "source": [
    "# All Done!\n",
    "\n",
    "Let's stop spark now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68acb25-9498-4e40-888a-6a20aef32bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hygiene\n",
    "# before we close the notebook, stop spark, otherwise Jupyter closes, but scala-spark keep going on...\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca54813-cca3-452c-a40c-185de971a2ed",
   "metadata": {},
   "source": [
    "# Up Next"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0abe53-7ae3-4d7e-9f29-ec3a9dc6fe69",
   "metadata": {},
   "source": [
    "Check out the exercises on real-world datasets next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94794f26-c617-4fa2-af6f-19992977db6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
